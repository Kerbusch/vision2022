{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Filter input cijfer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "num = 2\n",
    "\n",
    "x = list()\n",
    "\n",
    "for i in range(train_labels.shape[0]):\n",
    "    if train_labels[i] == num:\n",
    "        x.append(i)\n",
    "\n",
    "train_images = train_images[x]\n",
    "train_labels = train_labels[x]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Makes a model and imports an array with random values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "\n",
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n",
    "\n",
    "EPOCHS = 100\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n",
    "\n",
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    #display.clear_output(wait=True)\n",
    "    #generate_and_save_images(generator, epoch + 1, seed)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "  #display.clear_output(wait=True)\n",
    "  #generate_and_save_images(generator,epochs,seed)\n",
    "\n",
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 1 is 4.153169393539429 sec\n",
      "Time for epoch 2 is 2.3132259845733643 sec\n",
      "Time for epoch 3 is 2.3767948150634766 sec\n",
      "Time for epoch 4 is 2.3841028213500977 sec\n",
      "Time for epoch 5 is 2.398827314376831 sec\n",
      "Time for epoch 6 is 2.475964307785034 sec\n",
      "Time for epoch 7 is 2.5867607593536377 sec\n",
      "Time for epoch 8 is 2.704646110534668 sec\n",
      "Time for epoch 9 is 2.7049973011016846 sec\n",
      "Time for epoch 10 is 2.705477714538574 sec\n",
      "Time for epoch 11 is 2.6964356899261475 sec\n",
      "Time for epoch 12 is 2.697807788848877 sec\n",
      "Time for epoch 13 is 2.6958961486816406 sec\n",
      "Time for epoch 14 is 2.698279619216919 sec\n",
      "Time for epoch 15 is 2.697126626968384 sec\n",
      "Time for epoch 16 is 2.6942765712738037 sec\n",
      "Time for epoch 17 is 2.6941967010498047 sec\n",
      "Time for epoch 18 is 2.6961801052093506 sec\n",
      "Time for epoch 19 is 2.6965484619140625 sec\n",
      "Time for epoch 20 is 2.693629741668701 sec\n",
      "Time for epoch 21 is 2.6950080394744873 sec\n",
      "Time for epoch 22 is 2.6947109699249268 sec\n",
      "Time for epoch 23 is 2.695668935775757 sec\n",
      "Time for epoch 24 is 2.9647417068481445 sec\n",
      "Time for epoch 25 is 3.033907175064087 sec\n",
      "Time for epoch 26 is 3.013066053390503 sec\n",
      "Time for epoch 27 is 3.00530743598938 sec\n",
      "Time for epoch 28 is 2.9957611560821533 sec\n",
      "Time for epoch 29 is 2.968777656555176 sec\n",
      "Time for epoch 30 is 2.5490238666534424 sec\n",
      "Time for epoch 31 is 2.6872191429138184 sec\n",
      "Time for epoch 32 is 2.700758934020996 sec\n",
      "Time for epoch 33 is 2.697098731994629 sec\n",
      "Time for epoch 34 is 2.6963908672332764 sec\n",
      "Time for epoch 35 is 2.697105884552002 sec\n",
      "Time for epoch 36 is 2.6957900524139404 sec\n",
      "Time for epoch 37 is 2.6967859268188477 sec\n",
      "Time for epoch 38 is 2.6937975883483887 sec\n",
      "Time for epoch 39 is 2.6947968006134033 sec\n",
      "Time for epoch 40 is 2.6957898139953613 sec\n",
      "Time for epoch 41 is 2.6955747604370117 sec\n",
      "Time for epoch 42 is 2.6943156719207764 sec\n",
      "Time for epoch 43 is 2.6954286098480225 sec\n",
      "Time for epoch 44 is 2.6944119930267334 sec\n",
      "Time for epoch 45 is 2.695169687271118 sec\n",
      "Time for epoch 46 is 2.696638345718384 sec\n",
      "Time for epoch 47 is 2.690385103225708 sec\n",
      "Time for epoch 48 is 2.6830320358276367 sec\n",
      "Time for epoch 49 is 2.6461193561553955 sec\n",
      "Time for epoch 50 is 2.654733419418335 sec\n",
      "Time for epoch 51 is 2.658411979675293 sec\n",
      "Time for epoch 52 is 2.6521012783050537 sec\n",
      "Time for epoch 53 is 2.653773784637451 sec\n",
      "Time for epoch 54 is 2.659618616104126 sec\n",
      "Time for epoch 55 is 2.6540744304656982 sec\n",
      "Time for epoch 56 is 2.6515889167785645 sec\n",
      "Time for epoch 57 is 2.6275479793548584 sec\n",
      "Time for epoch 58 is 2.6400554180145264 sec\n",
      "Time for epoch 59 is 2.643035888671875 sec\n",
      "Time for epoch 60 is 2.644176483154297 sec\n",
      "Time for epoch 61 is 2.64854097366333 sec\n",
      "Time for epoch 62 is 2.639263391494751 sec\n",
      "Time for epoch 63 is 2.64277720451355 sec\n",
      "Time for epoch 64 is 2.6419386863708496 sec\n",
      "Time for epoch 65 is 2.644118547439575 sec\n",
      "Time for epoch 66 is 2.6407084465026855 sec\n",
      "Time for epoch 67 is 2.6439056396484375 sec\n",
      "Time for epoch 68 is 2.644871711730957 sec\n",
      "Time for epoch 69 is 2.644101619720459 sec\n",
      "Time for epoch 70 is 2.6428005695343018 sec\n",
      "Time for epoch 71 is 2.6419453620910645 sec\n",
      "Time for epoch 72 is 2.647608518600464 sec\n",
      "Time for epoch 73 is 2.6428956985473633 sec\n",
      "Time for epoch 74 is 2.6422064304351807 sec\n",
      "Time for epoch 75 is 2.6416056156158447 sec\n",
      "Time for epoch 76 is 2.645301580429077 sec\n",
      "Time for epoch 77 is 2.6441609859466553 sec\n",
      "Time for epoch 78 is 2.6464149951934814 sec\n",
      "Time for epoch 79 is 2.6459248065948486 sec\n",
      "Time for epoch 80 is 2.641937255859375 sec\n",
      "Time for epoch 81 is 2.644225835800171 sec\n",
      "Time for epoch 82 is 2.6443653106689453 sec\n",
      "Time for epoch 83 is 2.64103102684021 sec\n",
      "Time for epoch 84 is 2.63820219039917 sec\n",
      "Time for epoch 85 is 2.644282579421997 sec\n",
      "Time for epoch 86 is 2.6410489082336426 sec\n",
      "Time for epoch 87 is 2.644148826599121 sec\n",
      "Time for epoch 88 is 2.6418912410736084 sec\n",
      "Time for epoch 89 is 2.641784906387329 sec\n",
      "Time for epoch 90 is 2.6386446952819824 sec\n",
      "Time for epoch 91 is 2.640575408935547 sec\n",
      "Time for epoch 92 is 2.6448917388916016 sec\n",
      "Time for epoch 93 is 2.6431076526641846 sec\n",
      "Time for epoch 94 is 2.638540029525757 sec\n",
      "Time for epoch 95 is 2.6427485942840576 sec\n",
      "Time for epoch 96 is 2.639338970184326 sec\n",
      "Time for epoch 97 is 2.6393790245056152 sec\n",
      "Time for epoch 98 is 2.6422784328460693 sec\n",
      "Time for epoch 99 is 2.639986038208008 sec\n",
      "Time for epoch 100 is 2.641791820526123 sec\n"
     ]
    }
   ],
   "source": [
    "train(train_dataset, EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x18de894ad10>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "CODE from model.ipynb:\n",
    "ik heb hier de mijn code geplaatst om de afbeelding te kunnen catorizen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "1051/1051 [==============================] - 5s 4ms/step - loss: 0.8059 - accuracy: 0.9170 - val_loss: 0.1246 - val_accuracy: 0.9653\n",
      "Epoch 2/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0865 - accuracy: 0.9745 - val_loss: 0.1061 - val_accuracy: 0.9690\n",
      "Epoch 3/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0590 - accuracy: 0.9822 - val_loss: 0.0843 - val_accuracy: 0.9765\n",
      "Epoch 4/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0490 - accuracy: 0.9847 - val_loss: 0.1301 - val_accuracy: 0.9686\n",
      "Epoch 5/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0432 - accuracy: 0.9858 - val_loss: 0.1068 - val_accuracy: 0.9735\n",
      "Epoch 6/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0397 - accuracy: 0.9879 - val_loss: 0.0948 - val_accuracy: 0.9781\n",
      "Epoch 7/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0340 - accuracy: 0.9891 - val_loss: 0.1106 - val_accuracy: 0.9760\n",
      "Epoch 8/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0324 - accuracy: 0.9899 - val_loss: 0.1170 - val_accuracy: 0.9741\n",
      "Epoch 9/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0248 - accuracy: 0.9927 - val_loss: 0.1070 - val_accuracy: 0.9788\n",
      "Epoch 10/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0254 - accuracy: 0.9925 - val_loss: 0.1140 - val_accuracy: 0.9787\n",
      "Epoch 11/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0230 - accuracy: 0.9927 - val_loss: 0.1347 - val_accuracy: 0.9741\n",
      "Epoch 12/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0237 - accuracy: 0.9930 - val_loss: 0.1154 - val_accuracy: 0.9783\n",
      "Epoch 13/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0168 - accuracy: 0.9953 - val_loss: 0.1283 - val_accuracy: 0.9779\n",
      "Epoch 14/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0143 - accuracy: 0.9952 - val_loss: 0.1470 - val_accuracy: 0.9775\n",
      "Epoch 15/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0216 - accuracy: 0.9937 - val_loss: 0.1484 - val_accuracy: 0.9767\n",
      "Epoch 16/16\n",
      "1051/1051 [==============================] - 4s 4ms/step - loss: 0.0148 - accuracy: 0.9956 - val_loss: 0.1452 - val_accuracy: 0.9787\n",
      "1051/1051 - 2s - loss: 0.0119 - accuracy: 0.9965\n",
      "accuracy = 0.9965211749076843\n"
     ]
    }
   ],
   "source": [
    "digits_train = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "\n",
    "#input data for awnsering opdracht\n",
    "digits_test = pd.read_csv(\"data/test.csv\")\n",
    "test_data_in = digits_test.to_numpy()\n",
    "test_data = np.reshape(test_data_in, (test_data_in.shape[0], 28, 28))\n",
    "test_data = np.expand_dims(test_data, axis=3)\n",
    "\n",
    "#split traindata for train and validation data and labels\n",
    "msk = np.random.rand(len(digits_train)) < 0.8 # maak een random mask voor het splitsen tussen train data en verification data.\n",
    "#msk is een bool list die random zijn.\n",
    "train = digits_train[msk]\n",
    "val = digits_train[~msk]\n",
    "\n",
    "#to numpy\n",
    "train_data_in = train.to_numpy()\n",
    "val_data_in = val.to_numpy()\n",
    "\n",
    "\n",
    "# get labels\n",
    "train_labels = train_data_in[:, 0]  # get first colom from the training data\n",
    "val_labels = val_data_in[:, 0]  # get first colom from the training data\n",
    "train_data_without_label = train_data_in[:, 1:]  # remove first colom (labels) from the training data\n",
    "val_data_without_label = val_data_in[:, 1:]  # remove first colom (labels) from the training data\n",
    "\n",
    "\n",
    "# make a 3d array (size, 28, 28)\n",
    "train_data = np.reshape(train_data_without_label, (train_data_without_label.shape[0], 28, 28))\n",
    "val_data = np.reshape(val_data_without_label, (val_data_without_label.shape[0], 28, 28))\n",
    "\n",
    "# make proper dimensions\n",
    "train_data = np.expand_dims(train_data, axis=3)\n",
    "#test_data = np.expand_dims(test_data, axis=3)\n",
    "val_data = np.expand_dims(val_data, axis=3)\n",
    "\n",
    "# variables for model\n",
    "num_filters = 2\n",
    "filter_size = (3, 3)\n",
    "pool_size = (2, 2) # Deze pool_size is klein omdat de dataset klein (28x28) is. wanneer deze hoger is zullen er meer fouten in komen. (bijvoorbeeld bij een open 0 en een 6)\n",
    "\n",
    "model = Sequential([])\n",
    "\n",
    "model.add(Conv2D(10, (3, 3), padding='same', activation='relu', input_shape=train_data.shape[1:]))\n",
    "model.add(Conv2D(20, (3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=pool_size))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile('adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "h = model.fit(train_data, train_labels, epochs=16, validation_data=(val_data, val_labels))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(train_data,  train_labels, verbose=2)\n",
    "print(\"accuracy = {}\".format(test_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working: (1, 28, 28, 1), not (1, 28, 28, 1)\n",
      "[[7.9931997e-21 1.9564412e-23 1.0000000e+00 0.0000000e+00 4.4593285e-24\n",
      "  1.0776388e-29 2.5096827e-34 5.0330237e-21 1.8323470e-24 1.3294203e-25]]\n",
      "image contains: 2\n",
      "confident\n"
     ]
    }
   ],
   "source": [
    "s= tf.random.normal([1, noise_dim])\n",
    "p = generator(s)\n",
    "plt.imshow(p[0, :, :, 0], cmap='gray' )\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# x = np.expand_dims(p, axis=0)\n",
    "x = np.expand_dims(test_data[10], axis=0)\n",
    "print(\"working: {}, not {}\".format(x.shape, p.shape))\n",
    "prediction = model.predict(p* 127.5 + 127.5)\n",
    "print(prediction)\n",
    "n = np.argmax(prediction)\n",
    "print(\"image contains: {}\".format(n))\n",
    "if prediction[0][n] >= 1:\n",
    "    print(\"confident\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'image_at_epoch_0050.png'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [32]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mdisplay_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mEPOCHS\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [31]\u001B[0m, in \u001B[0;36mdisplay_image\u001B[1;34m(epoch_no)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdisplay_image\u001B[39m(epoch_no):\n\u001B[1;32m----> 2\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPIL\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage_at_epoch_\u001B[39;49m\u001B[38;5;132;43;01m{:04d}\u001B[39;49;00m\u001B[38;5;124;43m.png\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepoch_no\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\vision\\vision2022\\eindopdracht\\venv\\lib\\site-packages\\PIL\\Image.py:2953\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(fp, mode, formats)\u001B[0m\n\u001B[0;32m   2950\u001B[0m     filename \u001B[38;5;241m=\u001B[39m fp\n\u001B[0;32m   2952\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[1;32m-> 2953\u001B[0m     fp \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2954\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   2956\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'image_at_epoch_0050.png'"
     ]
    }
   ],
   "source": [
    "display_image(EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}